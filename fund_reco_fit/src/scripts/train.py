"""
é’ˆå¯¹å‰200åä¼˜åŒ–çš„è‡ªåŠ¨è°ƒå‚è„šæœ¬

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. ä¸“é—¨é’ˆå¯¹å‰200åè¿›è¡Œä¼˜åŒ–
2. å°è¯•ä¸åŒçš„ç‰¹å¾ç»„åˆ
3. æ‰¾åˆ°åœ¨å‰200åèŒƒå›´å†…å‘½ä¸­ç‡æœ€é«˜çš„æ¨¡å‹
"""

import sqlite3
import json
import numpy as np
import pandas as pd
from itertools import product
import time

# é…ç½®
DATABASE = "data/fundseeker_nav.db"
FEATURE_TABLE = "features_M_star"
TRAIN_START = "2021-01-01"
TRAIN_END = "2025-06-30"
TEST_DATE = "2025-12-31"
TOP_K = 200  # æ”¹ä¸º200

# ç‰¹å¾åˆ—
FEATURES = [
    "ret_1m", "ret_3m", "ret_6m", "ret_12m", "ret_24m", "ret_36m",
    "risk_adj_return", "downside_vol_36m", "mdd_36m", "morningstar_score",
    "momentum_ratio_3m_12m", "vol_trend_3m_6m", "drawdown_diff_6m_36m"
]

print("=" * 80)
print("ğŸ”§ é’ˆå¯¹å‰200åçš„è‡ªåŠ¨è°ƒå‚")
print("=" * 80)
print(f"è®­ç»ƒæœŸ: {TRAIN_START} åˆ° {TRAIN_END}")
print(f"æµ‹è¯•æœŸ: {TEST_DATE}")
print(f"é€‰æ‹©æ•°é‡: å‰{TOP_K}åªåŸºé‡‘")
print()

# è¿æ¥æ•°æ®åº“
conn = sqlite3.connect(DATABASE)

# åŠ è½½è®­ç»ƒæ•°æ®
print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
query = f"""
SELECT * FROM {FEATURE_TABLE}
WHERE DATE(snapshot_date) >= DATE(?) AND DATE(snapshot_date) <= DATE(?)
ORDER BY snapshot_date, fund_code
"""
train_df = pd.read_sql_query(query, conn, params=(TRAIN_START, TRAIN_END))
print(f"   è®­ç»ƒæ•°æ®: {len(train_df)} æ¡è®°å½•")

# æ·»åŠ æœªæ¥æ”¶ç›Šï¼ˆç”¨äºè®­ç»ƒï¼‰
print("   è®¡ç®—æœªæ¥æ”¶ç›Š...")
train_df = train_df.sort_values(["fund_code", "snapshot_date"])
train_df["future_ret_6m"] = train_df.groupby("fund_code")["ret_6m"].shift(-1)
train_df = train_df.dropna(subset=["future_ret_6m"])
print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(train_df)} æ¡")

# åŠ è½½æµ‹è¯•æ•°æ®
print("\nğŸ“ˆ åŠ è½½æµ‹è¯•æ•°æ®...")
test_df = pd.read_sql_query(
    f"SELECT * FROM {FEATURE_TABLE} WHERE DATE(snapshot_date) = DATE(?)",
    conn, params=(TEST_DATE,)
)
print(f"   æµ‹è¯•æ•°æ®: {len(test_df)} æ¡è®°å½•")

# è·å–çœŸå®çš„æœ€ä½³åŸºé‡‘ï¼ˆå‰200åï¼‰
actual_top = test_df.nlargest(TOP_K, "ret_6m")
actual_funds = set(actual_top["fund_code"].tolist())
print(f"   çœŸå®è¡¨ç°æœ€å¥½çš„å‰{TOP_K}åªåŸºé‡‘å·²ç¡®å®š")

# å®šä¹‰è¦å°è¯•çš„æƒé‡ç»„åˆ
print("\nğŸ”§ å‡†å¤‡å°è¯•ä¸åŒçš„æƒé‡ç»„åˆ...")
weight_candidates = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]

# ç®€åŒ–ï¼šåªè°ƒæ•´æœ€é‡è¦çš„å‡ ä¸ªç‰¹å¾
key_features = ["ret_3m", "ret_6m", "ret_12m", "risk_adj_return", "morningstar_score"]
print(f"   é‡ç‚¹è°ƒæ•´ç‰¹å¾: {key_features}")
print(f"   æ¯ä¸ªç‰¹å¾å°è¯• {len(weight_candidates)} ä¸ªæƒé‡å€¼")
print(f"   æ€»å…±éœ€è¦æµ‹è¯•: {len(weight_candidates) ** len(key_features)} ç§ç»„åˆ")
print("   ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...ï¼‰")

# å¼€å§‹æœç´¢æœ€ä½³æƒé‡
best_hit_rate = 0
best_weights = None
best_hits = 0
tested = 0

print("\nğŸ” å¼€å§‹æœç´¢æœ€ä½³æƒé‡...")
start_time = time.time()

for weight_combo in product(weight_candidates, repeat=len(key_features)):
    # æ„å»ºæƒé‡å­—å…¸
    weights = {feat: 0.05 for feat in FEATURES}  # é»˜è®¤æƒé‡
    for i, feat in enumerate(key_features):
        weights[feat] = weight_combo[i]

    # åœ¨è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—åˆ†æ•°
    feature_matrix = train_df[FEATURES].fillna(0).to_numpy()
    weight_vector = np.array([weights[feat] for feat in FEATURES])
    scores = feature_matrix.dot(weight_vector)

    train_df_copy = train_df.copy()
    train_df_copy["score"] = scores

    # åœ¨æµ‹è¯•æ•°æ®ä¸Šé¢„æµ‹
    test_feature_matrix = test_df[FEATURES].fillna(0).to_numpy()
    test_scores = test_feature_matrix.dot(weight_vector)
    test_df_copy = test_df.copy()
    test_df_copy["score"] = test_scores

    # é€‰å‡ºé¢„æµ‹çš„å‰200åª
    predicted_top = test_df_copy.nlargest(TOP_K, "score")
    predicted_funds = set(predicted_top["fund_code"].tolist())

    # è®¡ç®—å‘½ä¸­ç‡
    hits = predicted_funds & actual_funds
    hit_rate = len(hits) / TOP_K

    tested += 1

    # æ›´æ–°æœ€ä½³ç»“æœ
    if hit_rate > best_hit_rate:
        best_hit_rate = hit_rate
        best_weights = weights.copy()
        best_hits = len(hits)
        print(f"   âœ¨ æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹ï¼å‘½ä¸­ç‡: {hit_rate:.2%} ({len(hits)}/{TOP_K})")

    # æ¯æµ‹è¯•100ä¸ªç»„åˆæ˜¾ç¤ºè¿›åº¦
    if tested % 100 == 0:
        elapsed = time.time() - start_time
        print(f"   å·²æµ‹è¯• {tested} ä¸ªç»„åˆï¼Œè€—æ—¶ {elapsed:.1f}ç§’ï¼Œå½“å‰æœ€ä½³: {best_hit_rate:.2%}")

elapsed = time.time() - start_time
print(f"\nâœ… æœç´¢å®Œæˆï¼å…±æµ‹è¯• {tested} ä¸ªç»„åˆï¼Œè€—æ—¶ {elapsed:.1f}ç§’")

# æ˜¾ç¤ºæœ€ä½³ç»“æœ
print("\n" + "=" * 80)
print("ğŸ† æœ€ä½³æ¨¡å‹ç»“æœï¼ˆé’ˆå¯¹å‰200åï¼‰")
print("=" * 80)
print(f"å‘½ä¸­ç‡: {best_hit_rate:.2%} ({best_hits}/{TOP_K})")
print(f"\næœ€ä½³æƒé‡:")
for feat, weight in sorted(best_weights.items(), key=lambda x: x[1], reverse=True):
    if weight > 0:
        print(f"  {feat}: {weight:.3f}")

# ä¿å­˜æœ€ä½³æ¨¡å‹
output_file = "models/model_params_top200.json"
model_data = {
    "weights": best_weights,
    "hit_rate": best_hit_rate,
    "hit_count": best_hits,
    "total_predictions": TOP_K,
    "train_period": f"{TRAIN_START} to {TRAIN_END}",
    "test_date": TEST_DATE,
    "method": "grid_search_auto_tune_top200"
}

with open(output_file, "w") as f:
    json.dump(model_data, f, indent=2)

print(f"\nğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {output_file}")

# ç»“è®º
print("\n" + "=" * 80)
print("ğŸ’¡ ç»“è®º")
print("=" * 80)

random_baseline = TOP_K / 1112
print(f"éšæœºé€‰æ‹©åŸºçº¿å‘½ä¸­ç‡: {random_baseline:.2%}")
print(f"æ¨¡å‹å‘½ä¸­ç‡: {best_hit_rate:.2%}")
print(f"æ¨¡å‹æ¯”éšæœºé€‰æ‹©å¥½ {best_hit_rate / random_baseline:.1f} å€")

if best_hit_rate >= 0.25:
    print(f"\nâœ… å‘½ä¸­ç‡ {best_hit_rate:.2%} >= 25%ï¼Œæ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼")
    print("   å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å‹é¢„æµ‹2026å¹´å‰200å")
elif best_hit_rate >= 0.20:
    print(f"\nâš ï¸  å‘½ä¸­ç‡ {best_hit_rate:.2%} åœ¨20-25%ä¹‹é—´")
    print("   æ¨¡å‹æœ‰ä¸€å®šé¢„æµ‹èƒ½åŠ›ï¼Œä½†å»ºè®®è°¨æ…ä½¿ç”¨")
else:
    print(f"\nâŒ å‘½ä¸­ç‡ {best_hit_rate:.2%} < 20%")
    print("   æ¨¡å‹é¢„æµ‹èƒ½åŠ›æœ‰é™ï¼Œæ¥è¿‘éšæœºæ°´å¹³")

conn.close()
